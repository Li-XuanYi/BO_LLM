
"""
é…ç½®åŠ è½½å™¨
æ”¯æŒYAMLé…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œè¦†ç›–
"""

import yaml
import os
from typing import Dict, Any, Optional
import argparse


class Config:
    """é…ç½®ç®¡ç†ç±»"""
    
    def __init__(self, config_path: str = 'config.yaml'):
        """
        åˆå§‹åŒ–é…ç½®
        
        Parameters:
        -----------
        config_path : str
            é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.config = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        if os.path.exists(self.config_path):
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"âœ“ åŠ è½½é…ç½®æ–‡ä»¶: {self.config_path}")
            return config
        else:
            print(f"âš  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._default_config()
    
    def _default_config(self) -> Dict[str, Any]:
        """é»˜è®¤é…ç½®"""
        return {
            'llm': {
                'api_key': "sk-84ac2d321cf444e799ddc9db79c02e92",
                'base_url': "https://dashscope.aliyuncs.com/compatible-mode/v1",
                'model': 'qwen-plus',
                'temperature': {
                    'warm_start': 0.9,
                    'surrogate': 0.5,
                    'sampling_early': 0.7,
                    'sampling_mid': 0.6,
                    'sampling_late': 0.5
                },
                'max_tokens': {
                    'warm_start': 2500,
                    'surrogate': 2000,
                    'sampling': 1800
                }
            },
            'bayesian_optimization': {
                'init_points': 5,
                'n_iter': 25,
                'acquisition_type': 'PI',
                'xi': 0.01,
                'beta': 4.0,
                'n_candidates': 1000,
                'use_sensitivity': True,
                'random_state': 1
            },
            'parameter_bounds': {
                'current1': {'min': 3.0, 'max': 6.0},
                'charging_number': {'min': 5, 'max': 25},
                'current2': {'min': 1.0, 'max': 3.0}
            },
            'constraints': {
                'voltage_max': 4.2,
                'temp_max': 309,
                'target_soc': 0.8
            },
            'visualization': {
                'dpi': 300,
                'current_smooth_window': 5
            },
            'experiment': {
                'mode': 'llmbo',
                'save_results': True
            }
        }
    
    def get(self, key_path: str, default: Any = None) -> Any:
        """
        è·å–é…ç½®å€¼ï¼ˆæ”¯æŒç‚¹å·è·¯å¾„ï¼‰
        
        Parameters:
        -----------
        key_path : str
            é…ç½®é”®è·¯å¾„ï¼Œå¦‚ 'llm.model' æˆ– 'bayesian_optimization.init_points'
        default : any
            é»˜è®¤å€¼
            
        Returns:
        --------
        any : é…ç½®å€¼
        
        Example:
        --------
        >>> config.get('llm.model')
        'qwen-plus'
        >>> config.get('bayesian_optimization.n_iter')
        25
        """
        keys = key_path.split('.')
        value = self.config
        
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default
        
        return value
    
    def get_pbounds(self) -> Dict[str, tuple]:
        """è·å–å‚æ•°è¾¹ç•Œï¼ˆè½¬æ¢ä¸ºå…ƒç»„æ ¼å¼ï¼‰"""
        bounds = self.get('parameter_bounds', {})
        return {
            key: (val['min'], val['max'])
            for key, val in bounds.items()
        }
    
    def get_llm_config(self) -> Dict[str, Any]:
        """è·å–LLMé…ç½®"""
        return {
            'api_key': self.get('llm.api_key'),
            'base_url': self.get('llm.base_url'),
            'model': self.get('llm.model'),
            'temperature': self.get('llm.temperature'),
            'max_tokens': self.get('llm.max_tokens')
        }
    
    def get_bo_config(self) -> Dict[str, Any]:
        """è·å–è´å¶æ–¯ä¼˜åŒ–é…ç½®"""
        return self.get('bayesian_optimization', {})
    
    def get_constraints(self) -> Dict[str, Any]:
        """è·å–çº¦æŸæ¡ä»¶"""
        return self.get('constraints', {})
    
    def print_config(self):
        """æ‰“å°å½“å‰é…ç½®"""
        print("\n" + "="*60)
        print("å½“å‰ä¼˜åŒ–é…ç½®")
        print("="*60)
        
        print("\nğŸ“± LLMé…ç½®:")
        print(f"  æ¨¡å‹: {self.get('llm.model')}")
        print(f"  Temperature: Warm={self.get('llm.temperature.warm_start')}, "
              f"Surrogate={self.get('llm.temperature.surrogate')}, "
              f"Sampling={self.get('llm.temperature.sampling_early')}-{self.get('llm.temperature.sampling_late')}")
        
        print("\nğŸ”§ ä¼˜åŒ–é…ç½®:")
        print(f"  åˆå§‹ç‚¹: {self.get('bayesian_optimization.init_points')}")
        print(f"  è¿­ä»£æ¬¡æ•°: {self.get('bayesian_optimization.n_iter')}")
        print(f"  Acquisition: {self.get('bayesian_optimization.acquisition_type')}")
        print(f"  ä½¿ç”¨æ•æ„Ÿæ€§åˆ†æ: {self.get('bayesian_optimization.use_sensitivity')}")
        
        print("\nğŸ“Š å‚æ•°è¾¹ç•Œ:")
        for param, bounds in self.get_pbounds().items():
            print(f"  {param}: [{bounds[0]}, {bounds[1]}]")
        
        print("\nâš¡ çº¦æŸæ¡ä»¶:")
        constraints = self.get_constraints()
        print(f"  æœ€å¤§ç”µå‹: {constraints.get('voltage_max')} V")
        print(f"  æœ€å¤§æ¸©åº¦: {constraints.get('temp_max')} K")
        print(f"  ç›®æ ‡SOC: {constraints.get('target_soc')}")
        
        print("\n" + "="*60 + "\n")
    
    def override_from_args(self, args: argparse.Namespace):
        """ä»å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®"""
        if hasattr(args, 'model') and args.model:
            self.config['llm']['model'] = args.model
            print(f"âœ“ è¦†ç›–é…ç½®: LLMæ¨¡å‹ = {args.model}")
        
        if hasattr(args, 'init_points') and args.init_points:
            self.config['bayesian_optimization']['init_points'] = args.init_points
            print(f"âœ“ è¦†ç›–é…ç½®: åˆå§‹ç‚¹ = {args.init_points}")
        
        if hasattr(args, 'n_iter') and args.n_iter:
            self.config['bayesian_optimization']['n_iter'] = args.n_iter
            print(f"âœ“ è¦†ç›–é…ç½®: è¿­ä»£æ¬¡æ•° = {args.n_iter}")
        
        if hasattr(args, 'acquisition') and args.acquisition:
            self.config['bayesian_optimization']['acquisition_type'] = args.acquisition
            print(f"âœ“ è¦†ç›–é…ç½®: Acquisition = {args.acquisition}")


def parse_arguments() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='LLMBO - åŸºäºLLMå¢å¼ºçš„è´å¶æ–¯ä¼˜åŒ–',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤é…ç½®è¿è¡ŒLLMBO
  python main_LLMBO_v2.py
  
  # åªè¿è¡Œæ ‡å‡†BO
  python main_LLMBO_v2.py --mode bo
  
  # ä½¿ç”¨ä¸åŒæ¨¡å‹
  python main_LLMBO_v2.py --model qwen-max
  
  # è‡ªå®šä¹‰ä¼˜åŒ–å‚æ•°
  python main_LLMBO_v2.py --init-points 10 --n-iter 40
  
  # ä½¿ç”¨PI acquisition function
  python main_LLMBO_v2.py --acquisition PI
  
  # æŒ‡å®šé…ç½®æ–‡ä»¶
  python main_LLMBO_v2.py --config my_config.yaml
        """
    )
    
    parser.add_argument('--config', type=str, default='config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.yaml)')
    
    parser.add_argument('--mode', type=str, choices=['bo', 'llmbo', 'both'],
                       help='è¿è¡Œæ¨¡å¼: bo (æ ‡å‡†BO), llmbo (LLMBO), both (å¯¹æ¯”)')
    
    parser.add_argument('--model', type=str,
                       choices=['qwen-turbo', 'qwen-plus', 'qwen-max', 'gpt-4o', 'deepseek-r1'],
                       help='LLMæ¨¡å‹é€‰æ‹©')
    
    parser.add_argument('--init-points', type=int,
                       help='åˆå§‹é‡‡æ ·ç‚¹æ•°')
    
    parser.add_argument('--n-iter', type=int,
                       help='ä¼˜åŒ–è¿­ä»£æ¬¡æ•°')
    
    parser.add_argument('--acquisition', type=str, choices=['PI', 'EI', 'LCB'],
                       help='Acquisition functionç±»å‹')
    
    parser.add_argument('--no-sensitivity', action='store_true',
                       help='ç¦ç”¨å‚æ•°æ•æ„Ÿæ€§åˆ†æ')
    
    parser.add_argument('--seed', type=int,
                       help='éšæœºç§å­')
    
    parser.add_argument('-v', '--verbose', action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    
    return parser.parse_args()


def load_config_with_args() -> Config:
    """
    åŠ è½½é…ç½®å¹¶åº”ç”¨å‘½ä»¤è¡Œå‚æ•°
    
    Returns:
    --------
    Config : é…ç½®å¯¹è±¡
    """
    args = parse_arguments()
    config = Config(args.config)
    
    # åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–
    config.override_from_args(args)
    
    # åº”ç”¨é¢å¤–å‚æ•°
    if args.no_sensitivity:
        config.config['bayesian_optimization']['use_sensitivity'] = False
        print("âœ“ ç¦ç”¨å‚æ•°æ•æ„Ÿæ€§åˆ†æ")
    
    if args.seed is not None:
        config.config['bayesian_optimization']['random_state'] = args.seed
        print(f"âœ“ è®¾ç½®éšæœºç§å­ = {args.seed}")
    
    if args.verbose:
        config.config['logging']['level'] = 'DEBUG'
        config.config['debug']['verbose'] = True
        print("âœ“ å¯ç”¨è¯¦ç»†è¾“å‡ºæ¨¡å¼")
    
    # å¦‚æœæŒ‡å®šäº†modeï¼Œè¦†ç›–é…ç½®
    if args.mode:
        config.config['experiment']['mode'] = args.mode
    
    return config


if __name__ == "__main__":
    # æµ‹è¯•é…ç½®åŠ è½½
    config = load_config_with_args()
    config.print_config()
    
    print("\nç¤ºä¾‹è®¿é—®:")
    print(f"LLMæ¨¡å‹: {config.get('llm.model')}")
    print(f"å‚æ•°è¾¹ç•Œ: {config.get_pbounds()}")
    print(f"Acquisitionç±»å‹: {config.get('bayesian_optimization.acquisition_type')}")